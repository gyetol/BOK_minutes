{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 기사 크롤링\n",
    "\n",
    "언론사:연합뉴스,연합인포맥스,이데일리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "\n",
    "error_date=[]\n",
    "\n",
    "#본문내용크롤링\n",
    "def get_news(url):  \n",
    "    \n",
    "    header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "    html = requests.get(url, headers=header)\n",
    "    bsoup = BeautifulSoup(html.content, 'html.parser')\n",
    "    \n",
    "\n",
    "    # 기사 제목\n",
    "    title = bsoup.select('h3#articleTitle')[0].text\n",
    "\n",
    "   \n",
    "    # 날짜 \n",
    "    pdate = bsoup.select('.t11')[0].get_text()[:10]\n",
    "      \n",
    "\n",
    "    # 기사 본문\n",
    "    content = bsoup.select('#articleBodyContents')[0].get_text().replace('\\n', \" \")\n",
    "    ctext = content.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\") \n",
    "   \n",
    "    return title, pdate, ctext\n",
    "\n",
    "#네이버 접속\n",
    "def naver_news_crawling(query, start_date, end_date, s_from, e_to, page):\n",
    "    \n",
    "    title_list=[]\n",
    "    date_list=[]\n",
    "    contents_list=[]\n",
    "    \n",
    "    #최대5페이지까지\n",
    "    while page<50:\n",
    "  \n",
    "        #print(page)\n",
    "        url=\"https://search.naver.com/search.naver?&where=news&query=\"+query+\"&sm=tab_pge&sort=2&photo=0&field=0&reporter_article=&pd=3&ds=\"+start_date+\"&de=\"+ end_date +\"&docid=&nso=so:da,p:from\"+s_from+\"to\"+e_to+\",a:all&mynews=1&start=\"+str(page)+\"&refresh_start=0\"\n",
    "        #print(url)\n",
    "        \n",
    "        #header 추가 \n",
    "        header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "                  'cookie':'news_office_checked=1001,1018,2227'}\n",
    "    \n",
    "        req = requests.get(url,headers=header)\n",
    "        #print(url)\n",
    "        cont = req.text\n",
    "        soup = BeautifulSoup(cont, 'html.parser')\n",
    "\n",
    "        for urls in soup.select(\"a.info\"):\n",
    "            try :\n",
    "                if urls[\"href\"].startswith(\"https://news.naver.com\"):\n",
    "                   \n",
    "                    title_text, date_text, contents_text = get_news(urls[\"href\"])\n",
    "                    \n",
    "                    title_list.append(title_text)\n",
    "                    date_list.append(date_text)\n",
    "                    contents_list.append(contents_text)\n",
    "            except Exception as e:\n",
    "                print(e) \n",
    "                continue\n",
    "        page += 10\n",
    "        \n",
    "    result={}   \n",
    "    result= {'date' : date_list, 'title':title_list,'contents': contents_list}\n",
    "    \n",
    "    df=[]\n",
    "    df=pd.DataFrame(result)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#초기값 설정\n",
    "page = 1\n",
    "query = \"금리\" \n",
    "date = datetime.date(2009,1,1) #시작날짜\n",
    "start_date=date.strftime('%Y.%m.%d')\n",
    "end_date=start_date\n",
    "s_from = start_date.replace(\".\",\"\")\n",
    "e_to = start_date.replace(\".\",\"\")\n",
    "standard_date=start_date\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#---------------메인으로 돌아가는 부분----------------------\n",
    "while(standard_date!=\"2010.01.01\"):#끝날짜 \n",
    "    try:\n",
    "        \n",
    "        print(\"---------------\"+standard_date+\"날짜의 크롤링을 시작합니다---------------\")\n",
    "    \n",
    "        new_df = naver_news_crawling(query, start_date, end_date, s_from, e_to, page)\n",
    "        new_df=new_df.drop_duplicates()\n",
    "        new_df['title'] = new_df['title'].str.encode('utf-8', 'ignore').str.decode('utf-8')\n",
    "        new_df['contents'] = new_df['contents'].str.encode('utf-8', 'ignore').str.decode('utf-8')\n",
    "        new_df.to_csv(r\"C:\\Users\\HanBi\\Desktop\\2021\\project1\\crawling\\금리2009.csv\",encoding='utf-8',index=False, header=False, mode='a')\n",
    "    \n",
    "        date += datetime.timedelta(days=1)\n",
    "        start_date=date.strftime('%Y.%m.%d')\n",
    "        end_date=start_date\n",
    "    \n",
    "        s_from = start_date.replace(\".\",\"\")\n",
    "        e_to = start_date.replace(\".\",\"\")\n",
    "    \n",
    "        standard_date=start_date\n",
    "        print(\"성공\")\n",
    "    except Exception as e:\n",
    "        error_date.append(standard_date)\n",
    "        print(e)\n",
    "        print(\"실패\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "print(\"---------------크롤링을 종료합니다---------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
